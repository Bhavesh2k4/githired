{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQbB2Q5nRvzP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "    !pip install transformers accelerate datasets peft sqlglot\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import sqlite3\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "WPVTkwZrH7Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train.json\") as f:\n",
        "    train_bird = json.load(f)\n",
        "\n",
        "with open(\"dev.json\") as f:\n",
        "    dev_bird = json.load(f)\n",
        "\n",
        "print(\"BIRD Train:\", len(train_bird))\n",
        "print(\"BIRD Dev:\", len(dev_bird))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtkXXXSHH_sA",
        "outputId": "46addf52-a165-482c-f637-2b0b8d7cb1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIRD Train: 9428\n",
            "BIRD Dev: 1534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bird(item):\n",
        "    return {\n",
        "        \"instruction\": item[\"question\"],\n",
        "        \"output\": item[\"SQL\"],\n",
        "    }\n",
        "\n",
        "train_bird_prompts = [convert_bird(x) for x in train_bird]\n",
        "dev_bird_prompts = [convert_bird(x) for x in dev_bird]\n",
        "\n",
        "print(\"Converted BIRD:\", len(train_bird_prompts), len(dev_bird_prompts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5gYNLM-Ii-g",
        "outputId": "a16cbf92-35f7-4913-f7b8-9469696668b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted BIRD: 9428 1534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_placement.json\") as f:\n",
        "    train_place = json.load(f)\n",
        "\n",
        "with open(\"dev_placement.json\") as f:\n",
        "    dev_place = json.load(f)\n",
        "\n",
        "with open(\"dev_tied_append_placement.json\") as f:\n",
        "    dev_place_extra = json.load(f)\n",
        "\n",
        "dev_place = dev_place + dev_place_extra\n",
        "\n",
        "print(\"Placement Train:\", len(train_place))\n",
        "print(\"Placement Dev:\", len(dev_place))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsYU_gqoIlAu",
        "outputId": "b68f5a28-3550-41a6-f9f1-5fda937689fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placement Train: 20\n",
            "Placement Dev: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "placement_schema = \"\"\"\n",
        "users(id, email, role, is_approved)\n",
        "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
        "companies(id, user_id, company_name, industry, location)\n",
        "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
        "applications(id, student_id, job_id, status, applied_at)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "hGX4cP0wI3Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_place(item):\n",
        "    q = item[\"question\"]\n",
        "    sql = item[\"SQL\"]\n",
        "    return {\n",
        "        \"instruction\": f\"Given the following schema:\\n{placement_schema}\\nTranslate this question into SQL:\\n{q}\",\n",
        "        \"output\": sql,\n",
        "    }\n",
        "\n",
        "train_place_prompts = [convert_place(x) for x in train_place]\n",
        "dev_place_prompts = [convert_place(x) for x in dev_place]\n",
        "\n",
        "print(\"Converted Placement:\", len(train_place_prompts), len(dev_place_prompts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I0T8qrPI5ND",
        "outputId": "6d9b1320-9fc6-46fa-fb37-5f70b59aeb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted Placement: 20 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_all = train_bird_prompts + train_place_prompts\n",
        "dev_all = dev_bird_prompts + dev_place_prompts\n",
        "\n",
        "print(\"Total Training Samples:\", len(train_all))\n",
        "print(\"Total Dev Samples:\", len(dev_all))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg8ZTKnfI8Oa",
        "outputId": "d0d31233-8c86-41f8-cc11-96e070689abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Samples: 9448\n",
            "Total Dev Samples: 1542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_len = len(train_all)\n",
        "reduced_len = original_len // 10\n",
        "\n",
        "random.seed(42)\n",
        "train_all = random.sample(train_all, reduced_len)\n",
        "\n",
        "print(f\"Reduced training set from {original_len} → {len(train_all)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMBUfu_QPNdq",
        "outputId": "b7e88037-ecd2-4322-8ab8-dd0301afce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced training set from 9448 → 944 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "max_seq_length = 1024   # reduces training time\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMAOa3WBI_ig",
        "outputId": "ef780a85-d987-4a99-bd85-bf061d39b95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Mistral patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    texts = [\n",
        "        f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
        "        for instr, out in zip(batch[\"instruction\"], batch[\"output\"])\n",
        "    ]\n",
        "\n",
        "    enc = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seq_length,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
        "    return enc\n",
        "\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "dev_tokenized = dev_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=dev_dataset.column_names,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "13365f5b249a4c579d0da9fbf17fc9c6",
            "1e0dc6c1df7f4ae1ac4e7ca518c81ede",
            "2a2b2d3deb9a4990a2779f712e7a38e3",
            "0fff4b10079a474bb8cd226c4a2e0ac1",
            "1174175c0c2542ea894297a0f321598b",
            "444b852765a346b9bf1ed73e8ba83f69",
            "51c501cbfedb44b39d15a8380ac0ee1c",
            "557c5773012847fc9409db3c4016f293",
            "240f4e797fad4061a9401de6c727340e",
            "dbbcd34971f64eb287874f5cb5a96460",
            "2e56913fa25c41329d8ef198fbe79136",
            "f540860efcdb4e46a23596b92ba877b3",
            "51df1c4200b444e29507a69422481dfb",
            "4ca248eea335486cb20cb353e1b6934a",
            "300a437bb6ef41a19cf0dbe2a5f25892",
            "fe63d5518a2d4e61879801ef4793d13c",
            "fc81e1fee43a455696116543b2d53318",
            "cb581ecd9ab34ac6836e40bbe8bd92cc",
            "4582d95db1c74a2680275ce83488c699",
            "cdcdd20e07974e0eaef71c80ef2ecdd9",
            "7976e97f4e1a4aecac6511023bca4042",
            "79292e9dedbf4c0f8be6d963627eee72"
          ]
        },
        "id": "8E112cenJHv6",
        "outputId": "2936a0f1-eb5e-4f0d-d2a3-4087f625fa9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9448 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13365f5b249a4c579d0da9fbf17fc9c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1542 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f540860efcdb4e46a23596b92ba877b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"mistral-sql\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "\n",
        "    max_steps=200,      # TRAIN ONLY 200 STEPS\n",
        "    warmup_steps=20,\n",
        "\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHncSsrAJSUg",
        "outputId": "dbfd3d8c-afe1-4b10-b2d4-f4f9b1556da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"fine-tuned-mistral\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-mistral\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "tW-Akx3ZJZrQ",
        "outputId": "3a20d725-bc78-4560-add3-4f386712eb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 9,448 | Num Epochs = 1 | Total steps = 200\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 7,283,675,136 (0.58% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 1:09:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine-tuned-mistral/tokenizer_config.json',\n",
              " 'fine-tuned-mistral/special_tokens_map.json',\n",
              " 'fine-tuned-mistral/chat_template.jinja',\n",
              " 'fine-tuned-mistral/tokenizer.model',\n",
              " 'fine-tuned-mistral/added_tokens.json',\n",
              " 'fine-tuned-mistral/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(sql):\n",
        "    sql = sql.strip().lower()\n",
        "    sql = re.sub(r\"\\s+\", \" \", sql)\n",
        "    return sql.rstrip(\";\")\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return normalize(pred) == normalize(gold)\n",
        "\n",
        "def execution_accuracy(conn, pred, gold):\n",
        "    try:\n",
        "        p = conn.execute(pred).fetchall()\n",
        "        g = conn.execute(gold).fetchall()\n",
        "        return p == g\n",
        "    except:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "OLPbuYJOpE6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EM = 0\n",
        "total = len(dev_place_prompts)\n",
        "\n",
        "for sample in dev_place_prompts:\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    if exact_match(pred, gold):\n",
        "        EM += 1\n",
        "\n",
        "print(\"Exact Match Accuracy:\", EM / total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFqLmZh9pKbS",
        "outputId": "4cb12fe5-f6b5-4b57-fbcb-6a5115451bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def token_overlap(pred, gold):\n",
        "    p_tokens = set(normalize(pred).split())\n",
        "    g_tokens = set(normalize(gold).split())\n",
        "    if len(g_tokens) == 0:\n",
        "        return 0\n",
        "    return len(p_tokens & g_tokens) / len(g_tokens)\n",
        "\n",
        "overlap_scores = []\n",
        "\n",
        "for sample in dev_place_prompts:\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    overlap_scores.append(token_overlap(pred, gold))\n",
        "\n",
        "print(\"Token Overlap Accuracy:\", sum(overlap_scores) / len(overlap_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFIZwYGWplnb",
        "outputId": "167f1bf7-74b7-4288-eb0a-5c728149f4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Overlap Accuracy: 0.6300347222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlglot\n",
        "\n",
        "def structure_match(pred, gold):\n",
        "    try:\n",
        "        p_ast = sqlglot.parse_one(pred)\n",
        "        g_ast = sqlglot.parse_one(gold)\n",
        "        return p_ast == g_ast\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "SM = 0\n",
        "for sample in dev_place_prompts:\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    if structure_match(pred, gold):\n",
        "        SM += 1\n",
        "\n",
        "print(\"SQL Structure Accuracy:\", SM / len(dev_place_prompts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTZaNHUfpqNY",
        "outputId": "b712b989-78a2-49e9-aae1-aa8a78f6dbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL Structure Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlglot\n",
        "\n",
        "# ----------- Helper Functions -----------\n",
        "\n",
        "def normalize(sql):\n",
        "    sql = sql.strip().lower()\n",
        "    sql = re.sub(r\"\\s+\", \" \", sql)\n",
        "    return sql.rstrip(\";\")\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return normalize(pred) == normalize(gold)\n",
        "\n",
        "def token_overlap(pred, gold):\n",
        "    p = set(normalize(pred).split())\n",
        "    g = set(normalize(gold).split())\n",
        "    if len(g) == 0:\n",
        "        return 0\n",
        "    return len(p & g) / len(g)\n",
        "\n",
        "def structure_match(pred, gold):\n",
        "    try:\n",
        "        p_ast = sqlglot.parse_one(pred)\n",
        "        g_ast = sqlglot.parse_one(gold)\n",
        "        return p_ast == g_ast\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# ----------- Evaluation Loop -----------\n",
        "\n",
        "EM = 0\n",
        "SM = 0\n",
        "token_scores = []\n",
        "\n",
        "total = len(dev_place_prompts)\n",
        "\n",
        "for sample in dev_place_prompts:\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # Exact Match\n",
        "    if exact_match(pred, gold):\n",
        "        EM += 1\n",
        "\n",
        "    # SQL Structure Match\n",
        "    if structure_match(pred, gold):\n",
        "        SM += 1\n",
        "\n",
        "    # Token Overlap\n",
        "    token_scores.append(token_overlap(pred, gold))\n",
        "\n",
        "# ----------- Print Results -----------\n",
        "\n",
        "print(f\"Exact Match Accuracy:       {EM/total:.4f}\")\n",
        "print(f\"Token Overlap Accuracy:     {sum(token_scores)/total:.4f}\")\n",
        "print(f\"SQL Structure Accuracy:     {SM/total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN6Mz7y_1DLY",
        "outputId": "6b4567d9-30e5-4feb-f922-c9d83aa60e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Accuracy:       0.0000\n",
            "Token Overlap Accuracy:     0.6300\n",
            "SQL Structure Accuracy:     0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined DB-free evaluation cell\n",
        "import re\n",
        "import sqlglot\n",
        "import sqlglot.expressions as exp\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def normalize(sql: str):\n",
        "    if sql is None:\n",
        "        return \"\"\n",
        "    s = sql.strip()\n",
        "    # remove trailing semicolon and lower\n",
        "    s = re.sub(r\";\\s*$\", \"\", s.strip(), flags=re.MULTILINE)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower().strip()\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return normalize(pred) == normalize(gold)\n",
        "\n",
        "def token_overlap(pred, gold):\n",
        "    p = set(normalize(pred).split())\n",
        "    g = set(normalize(gold).split())\n",
        "    return 0.0 if len(g) == 0 else len(p & g) / len(g)\n",
        "\n",
        "def jaccard(pred, gold):\n",
        "    p = set(normalize(pred).split())\n",
        "    g = set(normalize(gold).split())\n",
        "    denom = len(p | g)\n",
        "    return 0.0 if denom == 0 else len(p & g) / denom\n",
        "\n",
        "def edit_ratio(pred, gold):\n",
        "    # difflib ratio in [0,1] where 1 = identical\n",
        "    return SequenceMatcher(None, normalize(pred), normalize(gold)).ratio()\n",
        "\n",
        "def ast_equal(pred, gold):\n",
        "    try:\n",
        "        p = sqlglot.parse_one(pred)\n",
        "        g = sqlglot.parse_one(gold)\n",
        "        return p == g\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def extract_tables(sql):\n",
        "    try:\n",
        "        t = sqlglot.parse_one(sql)\n",
        "        tables = {node.name.lower() for node in t.find_all(exp.Table)}\n",
        "        return tables\n",
        "    except Exception:\n",
        "        # fallback naive extraction from FROM/JOIN tokens\n",
        "        m = re.findall(r\"(?:from|join)\\s+([`\\\"]?[\\w\\.]+[`\\\"]?)\", sql, flags=re.I)\n",
        "        return {x.replace('\"','').replace(\"`\",\"\").split('.')[-1].lower() for x in m}\n",
        "\n",
        "def extract_columns(sql):\n",
        "    try:\n",
        "        t = sqlglot.parse_one(sql)\n",
        "        cols = set()\n",
        "        for c in t.find_all(exp.Column):\n",
        "            # column may come as table.col; keep both forms\n",
        "            col_sql = c.sql(dialect=None).lower()\n",
        "            # normalize quotes and schema qualifiers\n",
        "            col_sql = col_sql.replace('\"','').replace(\"`\",\"\")\n",
        "            cols.add(col_sql)\n",
        "        return cols\n",
        "    except Exception:\n",
        "        # naive: get words between select and from, split by commas\n",
        "        try:\n",
        "            sel = re.search(r\"select\\s+(.*?)\\s+from\\s\", sql, flags=re.I|re.S)\n",
        "            if not sel:\n",
        "                return set()\n",
        "            parts = [p.strip().lower() for p in sel.group(1).split(\",\")]\n",
        "            return {p for p in parts if p}\n",
        "        except Exception:\n",
        "            return set()\n",
        "\n",
        "def extract_where_clause(sql):\n",
        "    try:\n",
        "        t = sqlglot.parse_one(sql)\n",
        "        where = next(t.find_all(exp.Where), None)\n",
        "        if where is None:\n",
        "            return \"\"\n",
        "        return normalize(where.sql())\n",
        "    except Exception:\n",
        "        m = re.search(r\"where\\s+(.*?)(?:group by|order by|limit|$)\", sql, flags=re.I|re.S)\n",
        "        return normalize(m.group(1)) if m else \"\"\n",
        "\n",
        "def clause_match(pred, gold, clause):  # clause in [\"select\",\"from\",\"where\"]\n",
        "    try:\n",
        "        if clause == \"select\":\n",
        "            pcols = extract_columns(pred)\n",
        "            gcols = extract_columns(gold)\n",
        "            if not gcols:\n",
        "                return 0.0\n",
        "            return len(pcols & gcols) / len(gcols)\n",
        "        if clause == \"from\":\n",
        "            pt = extract_tables(pred)\n",
        "            gt = extract_tables(gold)\n",
        "            if not gt:\n",
        "                return 0.0\n",
        "            return len(pt & gt) / len(gt)\n",
        "        if clause == \"where\":\n",
        "            pw = extract_where_clause(pred)\n",
        "            gw = extract_where_clause(gold)\n",
        "            if not gw:\n",
        "                return 0.0\n",
        "            # simple token overlap in where clause\n",
        "            return token_overlap(pw, gw)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# keywords to check presence\n",
        "KEYWORDS = [\"select\",\"from\",\"where\",\"join\",\"group by\",\"order by\",\"limit\",\"having\"]\n",
        "\n",
        "def keyword_accuracy(pred, gold):\n",
        "    pred_norm = normalize(pred)\n",
        "    gold_norm = normalize(gold)\n",
        "    # we count keywords that are present in gold and check presence in pred\n",
        "    present = 0\n",
        "    total = 0\n",
        "    for kw in KEYWORDS:\n",
        "        if kw in gold_norm:\n",
        "            total += 1\n",
        "            if kw in pred_norm:\n",
        "                present += 1\n",
        "    return 1.0 if total == 0 else present/total\n",
        "\n",
        "# ---------------- Aggregation ----------------\n",
        "EM = 0\n",
        "AST = 0\n",
        "token_overlaps = []\n",
        "jaccards = []\n",
        "edit_scores = []\n",
        "select_scores = []\n",
        "from_scores = []\n",
        "where_scores = []\n",
        "kw_scores = []\n",
        "col_match_scores = []\n",
        "table_match_scores = []\n",
        "\n",
        "# For inspection\n",
        "DEBUG_N = 6\n",
        "debug_list = []\n",
        "\n",
        "# Iterate and evaluate\n",
        "for i, sample in enumerate(dev_place_prompts):\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    # generate\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "    pred_raw = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred_raw.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # metrics\n",
        "    if exact_match(pred, gold):\n",
        "        EM += 1\n",
        "    if ast_equal(pred, gold):\n",
        "        AST += 1\n",
        "\n",
        "    token_overlaps.append(token_overlap(pred, gold))\n",
        "    jaccards.append(jaccard(pred, gold))\n",
        "    edit_scores.append(edit_ratio(pred, gold))\n",
        "\n",
        "    s_match = clause_match(pred, gold, \"select\")\n",
        "    f_match = clause_match(pred, gold, \"from\")\n",
        "    w_match = clause_match(pred, gold, \"where\")\n",
        "    select_scores.append(s_match)\n",
        "    from_scores.append(f_match)\n",
        "    where_scores.append(w_match)\n",
        "\n",
        "    kw_scores.append(keyword_accuracy(pred, gold))\n",
        "\n",
        "    # columns/tables matching\n",
        "    gcols = extract_columns(gold)\n",
        "    pcols = extract_columns(pred)\n",
        "    col_match_scores.append(0.0 if not gcols else len(pcols & gcols) / len(gcols))\n",
        "\n",
        "    gtables = extract_tables(gold)\n",
        "    ptables = extract_tables(pred)\n",
        "    table_match_scores.append(0.0 if not gtables else len(ptables & gtables) / len(gtables))\n",
        "\n",
        "    # debug capture\n",
        "    if i < DEBUG_N:\n",
        "        debug_list.append({\n",
        "            \"idx\": i,\n",
        "            \"prompt\": prompt,\n",
        "            \"gold\": gold,\n",
        "            \"pred\": pred,\n",
        "            \"select_match\": s_match,\n",
        "            \"from_match\": f_match,\n",
        "            \"where_match\": w_match,\n",
        "            \"token_overlap\": token_overlaps[-1],\n",
        "            \"jaccard\": jaccards[-1],\n",
        "            \"edit_ratio\": edit_scores[-1],\n",
        "            \"ast_equal\": ast_equal(pred, gold)\n",
        "        })\n",
        "\n",
        "# ------------- Print Summary -------------\n",
        "n = len(dev_place_prompts)\n",
        "print(\"----- Aggregate Scores -----\")\n",
        "print(f\"Total samples evaluated: {n}\")\n",
        "print(f\"Exact Match (EM): {EM}/{n} = {EM/n:.4f}\")\n",
        "print(f\"AST Structure Match: {AST}/{n} = {AST/n:.4f}\")\n",
        "print(f\"Mean Token Overlap: {sum(token_overlaps)/n:.4f}\")\n",
        "print(f\"Mean Jaccard: {sum(jaccards)/n:.4f}\")\n",
        "print(f\"Mean Edit Ratio (difflib): {sum(edit_scores)/n:.4f}\")\n",
        "print()\n",
        "print(\"Clause partial matches (mean):\")\n",
        "print(f\"  SELECT: {sum(select_scores)/n:.4f}\")\n",
        "print(f\"  FROM:   {sum(from_scores)/n:.4f}\")\n",
        "print(f\"  WHERE:  {sum(where_scores)/n:.4f}\")\n",
        "print()\n",
        "print(f\"Mean Keyword Accuracy: {sum(kw_scores)/n:.4f}\")\n",
        "print(f\"Mean Column Match: {sum(col_match_scores)/n:.4f}\")\n",
        "print(f\"Mean Table Match: {sum(table_match_scores)/n:.4f}\")\n",
        "\n",
        "# ------------- Print debug examples -------------\n",
        "print(\"\\n----- Example outputs (first few) -----\")\n",
        "for d in debug_list:\n",
        "    print(f\"\\nSample #{d['idx']+1}\")\n",
        "    print(\"PROMPT:\", d[\"prompt\"][:180].replace(\"\\n\",\" \") + \"...\")\n",
        "    print(\"GOLD :\", d[\"gold\"])\n",
        "    print(\"PRED :\", d[\"pred\"])\n",
        "    print(f\"SELECT match: {d['select_match']:.3f} | FROM match: {d['from_match']:.3f} | WHERE match: {d['where_match']:.3f}\")\n",
        "    print(f\"Token overlap: {d['token_overlap']:.3f} | Jaccard: {d['jaccard']:.3f} | Edit ratio: {d['edit_ratio']:.3f}\")\n",
        "    print(f\"AST equal: {d['ast_equal']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkHqQcu3iHO",
        "outputId": "41513c54-bde5-4a5c-95dc-57c94c84dde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Aggregate Scores -----\n",
            "Total samples evaluated: 8\n",
            "Exact Match (EM): 0/8 = 0.0000\n",
            "AST Structure Match: 0/8 = 0.0000\n",
            "Mean Token Overlap: 0.6595\n",
            "Mean Jaccard: 0.1271\n",
            "Mean Edit Ratio (difflib): 0.1630\n",
            "\n",
            "Clause partial matches (mean):\n",
            "  SELECT: 0.0455\n",
            "  FROM:   0.8750\n",
            "  WHERE:  0.4643\n",
            "\n",
            "Mean Keyword Accuracy: 0.9688\n",
            "Mean Column Match: 0.0455\n",
            "Mean Table Match: 0.8750\n",
            "\n",
            "----- Example outputs (first few) -----\n",
            "\n",
            "Sample #1\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT DISTINCT c.company_name FROM companies c JOIN jobs j ON c.id = j.company_id WHERE MONTH(j.created_at) = 10\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "Show me all companies that visited in October 2021 and have more than 100 employees.\n",
            "\n",
            "```sql\n",
            "SELECT companies.company_name, companies.industry, companies.location\n",
            "FROM companies\n",
            "WHERE EXISTS (\n",
            "    SELECT 1\n",
            "    FROM jobs\n",
            "    WHERE jobs.created_at >= '2021-10-01' AND jobs.created_at <= '2021-10-31'\n",
            "    AND jobs.company_id = companies.id\n",
            "    AND (\n",
            "        SELECT COUNT(*)\n",
            "        FROM employees\n",
            "        WHERE employees.company_id = companies.id\n",
            "    ) > 100\n",
            ")\n",
            "```\n",
            "SELECT match: 0.000 | FROM match: 1.000 | WHERE match: 0.500\n",
            "Token overlap: 0.375 | Jaccard: 0.071 | Edit ratio: 0.171\n",
            "AST equal: False\n",
            "\n",
            "Sample #2\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT COUNT(*) FROM students WHERE department = 'Electronics'\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "How many students are from Electronics department?\n",
            "\n",
            "```sql\n",
            "SELECT COUNT(*)\n",
            "FROM students\n",
            "WHERE department = 'Electronics';\n",
            "```\n",
            "SELECT match: 0.000 | FROM match: 1.000 | WHERE match: 0.750\n",
            "Token overlap: 1.000 | Jaccard: 0.160 | Edit ratio: 0.226\n",
            "AST equal: False\n",
            "\n",
            "Sample #3\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT j.title, c.company_name FROM jobs j JOIN companies c ON j.company_id = c.id WHERE j.location = 'Bangalore'\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "Find all job positions in Bangalore with a CTC greater than 500000 and the number of applications for each job position.\n",
            "\n",
            "```sql\n",
            "SELECT jobs.title, COUNT(applications.id) as num_applications\n",
            "FROM jobs\n",
            "JOIN companies ON jobs.company_id = companies.id\n",
            "WHERE companies.location = 'Bangalore'\n",
            "GROUP BY jobs.title\n",
            "HAVING jobs.ctc_offered > 500000\n",
            "ORDER BY jobs.ctc_offered DESC;\n",
            "```\n",
            "\n",
            "This query will return the title of the job positions in Bangalore with a CTC greater than 500000 and the number\n",
            "SELECT match: 0.000 | FROM match: 1.000 | WHERE match: 0.500\n",
            "Token overlap: 0.562 | Jaccard: 0.099 | Edit ratio: 0.203\n",
            "AST equal: False\n",
            "\n",
            "Sample #4\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT s.first_name, s.last_name, c.company_name FROM students s JOIN applications a ON s.id = a.student_id JOIN jobs j ON a.job_id = j.id JOIN companies c ON j.company_id = c.id WHERE a.status = 'offered'\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "Show me students who got offers from companies in the IT industry and have a CGPA greater than 3.5.\n",
            "\n",
            "```sql\n",
            "SELECT students.id, students.first_name, students.last_name, students.student_id, students.department\n",
            "FROM students\n",
            "JOIN companies ON students.user_id = companies.user_id\n",
            "JOIN jobs ON companies.id = jobs.company_id\n",
            "WHERE companies.industry = 'IT'\n",
            "AND students.cgpa > 3.5;\n",
            "```\n",
            "\n",
            "This query will return the ID, first name, last name, student ID, and department of students who have applied to jobs in IT industry companies and have a CGPA greater\n",
            "SELECT match: 0.000 | FROM match: 0.750 | WHERE match: 0.250\n",
            "Token overlap: 0.400 | Jaccard: 0.098 | Edit ratio: 0.052\n",
            "AST equal: False\n",
            "\n",
            "Sample #5\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT AVG(s.cgpa) FROM students s JOIN applications a ON s.id = a.student_id WHERE a.status = 'selected'\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "What is the average CGPA of placed students?\n",
            "\n",
            "```sql\n",
            "SELECT AVG(s.cgpa)\n",
            "FROM students s\n",
            "JOIN applications a ON s.id = a.student_id\n",
            "JOIN jobs j ON a.job_id = j.id\n",
            "WHERE j.status = 'placed';\n",
            "```\n",
            "\n",
            "This query will return the average CGPA of students who have been placed in jobs. The query uses a join to connect the students table with the applications table, and then another join to connect the applications table with the jobs table. The WHERE clause is used to filter the results to only include students who have been placed in jobs. The AVG function is then used to calculate the average CGPA of these students.\n",
            "SELECT match: 0.000 | FROM match: 1.000 | WHERE match: 0.500\n",
            "Token overlap: 0.867 | Jaccard: 0.138 | Edit ratio: 0.185\n",
            "AST equal: False\n",
            "\n",
            "Sample #6\n",
            "PROMPT: Given the following schema:  users(id, email, role, is_approved) students(id, user_id, first_name, last_name, student_id, department, cgpa) companies(id, user_id, company_name, ind...\n",
            "GOLD : SELECT j.title, c.company_name, j.ctc_offered FROM jobs j JOIN companies c ON j.company_id = c.id WHERE j.title LIKE '%data%' OR j.title LIKE '%analyst%'\n",
            "PRED : Given the following schema:\n",
            "\n",
            "users(id, email, role, is_approved)\n",
            "students(id, user_id, first_name, last_name, student_id, department, cgpa)\n",
            "companies(id, user_id, company_name, industry, location)\n",
            "jobs(id, company_id, title, description, job_type, ctc_offered, created_at)\n",
            "applications(id, student_id, job_id, status, applied_at)\n",
            "\n",
            "Translate this question into SQL:\n",
            "Show me all data science jobs that have been applied for by students with a CGPA greater than 3.5 and who belong to the Computer Science department.\n",
            "\n",
            "```sql\n",
            "SELECT jobs.id, jobs.title, jobs.description, jobs.job_type, jobs.ctc_offered,\n",
            "       students.first_name, students.last_name, students.department, students.cgpa\n",
            "FROM jobs\n",
            "JOIN applications ON jobs.id = applications.job_id\n",
            "JOIN students ON applications.student_id = students.id\n",
            "WHERE students.department = 'Computer Science'\n",
            "  AND students.cgpa > 3.5\n",
            "  AND jobs.job_type =\n",
            "SELECT match: 0.000 | FROM match: 0.500 | WHERE match: 0.000\n",
            "Token overlap: 0.350 | Jaccard: 0.071 | Edit ratio: 0.153\n",
            "AST equal: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EM = 0\n",
        "EX = 0\n",
        "total = len(dev_place_prompts)\n",
        "\n",
        "conn = sqlite3.connect(\"placement.sqlite\")  # your DB with placement schema\n",
        "\n",
        "for sample in dev_place_prompts:\n",
        "    gold = sample[\"output\"]\n",
        "    prompt = sample[\"instruction\"]\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    if exact_match(pred, gold):\n",
        "        EM += 1\n",
        "\n",
        "    if execution_accuracy(conn, pred, gold):\n",
        "        EX += 1\n",
        "\n",
        "print(\"Exact Match Accuracy:\", EM / total)\n",
        "print(\"Execution Accuracy:\", EX / total)\n",
        "print(\"Final Evaluation Accuracy:\", (EM/total + EX/total) / 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5H8a-533vMy",
        "outputId": "3a2c571a-a487-4793-eca6-e67a79b9aca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Accuracy: 0.0\n",
            "Execution Accuracy: 0.0\n",
            "Final Evaluation Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(query, schema):\n",
        "    prompt = f\"### Instruction:\\nGiven the schema:\\n{schema}\\nTranslate this question into SQL:\\n{query}\\n\\n### Response:\\n\"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**enc, max_new_tokens=150, do_sample=False)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
        "\n",
        "ask(\"List all students with CGPA above 8\", placement_schema)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cJlh5ZZlpwql",
        "outputId": "c954cf85-6443-431f-9766-65bedf79224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```sql\\nSELECT s.id, s.user_id, s.first_name, s.last_name, s.student_id, s.department, s.cgpa\\nFROM students s\\nWHERE s.cgpa > 8;\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5QK88F63udU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fine-tuned-mistral.zip fine-tuned-mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2YBLO47prxK",
        "outputId": "a0a80b37-8fcc-4e28-9989-edf0b9c34cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fine-tuned-mistral/ (stored 0%)\n",
            "  adding: fine-tuned-mistral/README.md (deflated 65%)\n",
            "  adding: fine-tuned-mistral/tokenizer.json (deflated 85%)\n",
            "  adding: fine-tuned-mistral/tokenizer.model (deflated 55%)\n",
            "  adding: fine-tuned-mistral/adapter_model.safetensors (deflated 7%)\n",
            "  adding: fine-tuned-mistral/tokenizer_config.json (deflated 68%)\n",
            "  adding: fine-tuned-mistral/special_tokens_map.json (deflated 79%)\n",
            "  adding: fine-tuned-mistral/adapter_config.json (deflated 57%)\n",
            "  adding: fine-tuned-mistral/chat_template.jinja (deflated 64%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3X9Tu6399s1",
        "outputId": "b58c2f87-9134-4b74-c053-5886302a9993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.50.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtARLCt2_kOn",
        "outputId": "e4be04bc-ec87-44fa-9cdc-34ea7120e46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 33CPxN7YfOrNiC6gDQWv2rM7LEi_2Q36iGpiXACw5m73poxTx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXhZzjEo_p0z",
        "outputId": "f8a4db13-54a4-4b14-ac9f-539645be9fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://2e3d7bcda07f.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.168.212.77:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T16:39:08+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "2025-09-25 16:40:44.880170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758818444.917821   22952 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758818444.929311   22952 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758818444.957946   22952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758818444.957985   22952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758818444.957993   22952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758818444.957999   22952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.7: Fast Mistral patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new tunnel\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run /content/app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um7RwZWd_thQ",
        "outputId": "7c50f739-f29d-4355-eca8-fde6d3c17254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP8Cy5eA_9S1",
        "outputId": "25701c2f-9101-4578-be69-60d087a9355e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: fine-tuned-mistral/ (stored 0%)\n",
            "  adding: fine-tuned-mistral/adapter_model.safetensors (deflated 8%)\n",
            "  adding: fine-tuned-mistral/README.md (deflated 65%)\n",
            "  adding: fine-tuned-mistral/tokenizer.model (deflated 55%)\n",
            "  adding: fine-tuned-mistral/adapter_config.json (deflated 58%)\n",
            "  adding: fine-tuned-mistral/tokenizer_config.json (deflated 68%)\n",
            "  adding: fine-tuned-mistral/special_tokens_map.json (deflated 79%)\n",
            "  adding: fine-tuned-mistral/tokenizer.json (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r fine-tuned-mistral.zip fine-tuned-mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UrhBksfBCqXD",
        "outputId": "69669b42-24b6-44b9-aef6-7e254dcd341d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_a7f39751-07c7-45b2-b983-93af93f10dda\", \"fine-tuned-mistral.zip\", 155769979)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"fine-tuned-mistral.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrquxfxvC0ny"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13365f5b249a4c579d0da9fbf17fc9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e0dc6c1df7f4ae1ac4e7ca518c81ede",
              "IPY_MODEL_2a2b2d3deb9a4990a2779f712e7a38e3",
              "IPY_MODEL_0fff4b10079a474bb8cd226c4a2e0ac1"
            ],
            "layout": "IPY_MODEL_1174175c0c2542ea894297a0f321598b"
          }
        },
        "1e0dc6c1df7f4ae1ac4e7ca518c81ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444b852765a346b9bf1ed73e8ba83f69",
            "placeholder": "​",
            "style": "IPY_MODEL_51c501cbfedb44b39d15a8380ac0ee1c",
            "value": "Map: 100%"
          }
        },
        "2a2b2d3deb9a4990a2779f712e7a38e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_557c5773012847fc9409db3c4016f293",
            "max": 9448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_240f4e797fad4061a9401de6c727340e",
            "value": 9448
          }
        },
        "0fff4b10079a474bb8cd226c4a2e0ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbbcd34971f64eb287874f5cb5a96460",
            "placeholder": "​",
            "style": "IPY_MODEL_2e56913fa25c41329d8ef198fbe79136",
            "value": " 9448/9448 [00:07&lt;00:00, 1432.61 examples/s]"
          }
        },
        "1174175c0c2542ea894297a0f321598b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "444b852765a346b9bf1ed73e8ba83f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c501cbfedb44b39d15a8380ac0ee1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "557c5773012847fc9409db3c4016f293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "240f4e797fad4061a9401de6c727340e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbbcd34971f64eb287874f5cb5a96460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e56913fa25c41329d8ef198fbe79136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f540860efcdb4e46a23596b92ba877b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51df1c4200b444e29507a69422481dfb",
              "IPY_MODEL_4ca248eea335486cb20cb353e1b6934a",
              "IPY_MODEL_300a437bb6ef41a19cf0dbe2a5f25892"
            ],
            "layout": "IPY_MODEL_fe63d5518a2d4e61879801ef4793d13c"
          }
        },
        "51df1c4200b444e29507a69422481dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc81e1fee43a455696116543b2d53318",
            "placeholder": "​",
            "style": "IPY_MODEL_cb581ecd9ab34ac6836e40bbe8bd92cc",
            "value": "Map: 100%"
          }
        },
        "4ca248eea335486cb20cb353e1b6934a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4582d95db1c74a2680275ce83488c699",
            "max": 1542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdcdd20e07974e0eaef71c80ef2ecdd9",
            "value": 1542
          }
        },
        "300a437bb6ef41a19cf0dbe2a5f25892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7976e97f4e1a4aecac6511023bca4042",
            "placeholder": "​",
            "style": "IPY_MODEL_79292e9dedbf4c0f8be6d963627eee72",
            "value": " 1542/1542 [00:01&lt;00:00, 1501.74 examples/s]"
          }
        },
        "fe63d5518a2d4e61879801ef4793d13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc81e1fee43a455696116543b2d53318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb581ecd9ab34ac6836e40bbe8bd92cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4582d95db1c74a2680275ce83488c699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcdd20e07974e0eaef71c80ef2ecdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7976e97f4e1a4aecac6511023bca4042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79292e9dedbf4c0f8be6d963627eee72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}